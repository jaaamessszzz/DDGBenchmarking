{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DDG Stuff\n",
    "This is the iPython Notebook containing a lot of my code for working with the Zemu DDG dataset. Each entry has its own little description below.\n",
    "##Contents:\n",
    "* [RunScript](#RunScript)<br>\n",
    "* [RosettaOut_Parser](#RosettaOut_Parser.py)<br>\n",
    "* [List_PDBs](#List_PDBs.py)<br>\n",
    "* [PDB_REDO .pdb Scraper](#PDB_REDO-.pdb-Scraper)<br>\n",
    "* [JSON File Editor](#JSON-File-Editor)<br>\n",
    "* [PDB_REDO Data Setup](#PDB_REDO-data-setup)<br>\n",
    "* [Resfile_Editor](#Resfile_Editor.py)<br>\n",
    "* [Strip PDB_REDO .pdb's](#Strip-PDBs)<br>\n",
    "<br>\n",
    "* [Scratch Space](#Scratch-Space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##RunScript\n",
    "This is a modified version of SubmitRun_DDG_Zemu_General.py for locally troubleshooting RosettaScripts (specifically DDG_Text.xml). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Modified SubmitRun_DDG_Zemu_General.py for local troubleshooting of DDG_Test.xml\n",
    "#3/26/16\n",
    "\n",
    "from Bio.PDB import *\n",
    "from datetime import *\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "import shutil\n",
    "import inspect\n",
    "import gzip\n",
    "import tempfile\n",
    "import re\n",
    "import json\n",
    "\n",
    "os.chdir('/Users/jameslucas/Kortemme_Rotation/')\n",
    "\n",
    "def read_mutations_resfile(filenum_dir):\n",
    "    resfile = os.path.join(filenum_dir, 'mutations_repack.resfile')\n",
    "    mutations = []\n",
    "    with open(resfile, 'r') as f:\n",
    "        post_start = False\n",
    "        for line in f:\n",
    "            if post_start:\n",
    "                line = line.strip()\n",
    "                pdb_resnum, chain, pikaa, mut_res = line.split()\n",
    "                mutations.append( [pdb_resnum, chain, pikaa, mut_res] )\n",
    "            elif line.startswith('start'):\n",
    "                post_start = True\n",
    "    return mutations\n",
    "\n",
    "def find_neighbors(filenum_dir, pdb_path, neighbor_distance = 8.0):\n",
    "    mutations = read_mutations_resfile(filenum_dir)\n",
    "    open_filename = pdb_path\n",
    "    parser = PDBParser(PERMISSIVE=1)\n",
    "    open_strct = parser.get_structure('Open', open_filename)\n",
    "\n",
    "    # There should only be one model in PDB file\n",
    "    num_models = 0\n",
    "    for model in open_strct.get_models():\n",
    "        num_models += 1\n",
    "    assert( num_models == 1 )\n",
    "\n",
    "    chain_list = [chain.get_id() for chain in open_strct[0].get_chains()]\n",
    "    neighbors = set()\n",
    "    for mutation in mutations:\n",
    "        res_id, chain_id, pikaa, mut_aa = mutation\n",
    "        mut_chain = str(chain_id)\n",
    "        try:\n",
    "            mut_pos = int( res_id )\n",
    "            mut_insertion_code = ' '\n",
    "        except ValueError:\n",
    "            mut_pos = int( res_id[:-1] )\n",
    "            mut_insertion_code = res_id[-1]\n",
    "\n",
    "        mut_residue = open_strct[0][mut_chain][(' ', mut_pos, mut_insertion_code)]\n",
    "        for chain in chain_list:\n",
    "            for residue in [res.get_id() for res in open_strct[0][chain].get_residues()]:\n",
    "                try:\n",
    "                    # Kyle note - might be good to do something else for consistency, since not all residues have CB\n",
    "                    dist = mut_residue['CB'] - open_strct[0][chain][residue]['CB']\n",
    "                    if dist < neighbor_distance:\n",
    "                        neighbors.add( (residue, chain) )\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "    return neighbors\n",
    "\n",
    "#Parses dataset .json file and outputs chain to move and input PDB file directory\n",
    "def json_parser():\n",
    "    jsonload = open(\"data/blank_job_dict.json\")\n",
    "    jsonfile = json.load(jsonload)\n",
    "    key = sorted(jsonfile.keys())[0] #SGE_TASK_ID\n",
    "    chaintomove = jsonfile[key][\"%%chainstomove%%\"]\n",
    "    inputdir = jsonfile[key]['input_file_list'][0]\n",
    "    print key\n",
    "    return chaintomove, inputdir\n",
    "\n",
    "#Finds neighbors within 8A and adds position and Chain information to a list\n",
    "def neighbors_list(pdb_filepath, pdb_file):\n",
    "    neighbors = find_neighbors(pdb_filepath, pdb_file, 8)\n",
    "    pivotlist = ''\n",
    "    for i in neighbors:\n",
    "        pivotlist = pivotlist + str(i[0][1]) + str(i[1]) + ','\n",
    "    pivotlist = pivotlist[:-1]\n",
    "    return pivotlist\n",
    "\n",
    "#Reads resfile and returns mutation position+chain and type\n",
    "def resfile_stuff(pdb_filepath):\n",
    "    resfile = read_mutations_resfile(pdb_filepath)\n",
    "    position = []\n",
    "    for i in resfile:\n",
    "        position.append(i[0] + i[1])\n",
    "    return position\n",
    "    \n",
    "#Prints CMD input with PDBID, associated mutation, and pivot residues\n",
    "def bash(chaintomove, inputdir, outputdir):\n",
    "    #Removes PDB file from path, saves in variable filenum_dir\n",
    "    inputdir_parse = re.sub(\"/\",' ', str(inputdir))\n",
    "    data, filenum, pdbtemp = inputdir_parse.split()\n",
    "    filenum_dir = data + \"/\" + filenum\n",
    "    PDBID = pdbtemp[:-4]\n",
    "    predIDoutdir = outputdir + filenum\n",
    "    \n",
    "    #Makes a folder for data dumping\n",
    "    print 'Making directory %s%s...' %(outputdir, filenum)\n",
    "    os.makedirs(predIDoutdir)\n",
    "  \n",
    "    #Assigns function output to variables for bash input (pivot_residues, target, resfile_relpath)\n",
    "    target = resfile_stuff(filenum_dir)\n",
    "    pivot_residues = neighbors_list(filenum_dir, inputdir)\n",
    "    resfile_relpath = os.path.relpath(filenum_dir, predIDoutdir)\n",
    "    pdb_relpath = os.path.relpath(inputdir, predIDoutdir)\n",
    "    \n",
    "    targetlist = ''\n",
    "    for i in target:\n",
    "        targetlist = targetlist + i + ','\n",
    "    targetlist = targetlist[:-1]\n",
    "    \n",
    "    print os.getcwd()\n",
    "    \n",
    "    arg = ['/Users/jameslucas/Rosetta/main/source/bin/rosetta_scripts.macosclangrelease',\n",
    "           '-s',\n",
    "           pdb_relpath,\n",
    "           '-parser:protocol',\n",
    "           '../../RosettaScripts/DDG_Test.xml',\n",
    "           '-ignore_unrecognized_res',\n",
    "           '-parser:script_vars',\n",
    "           'resfile_relpath=%s' %(resfile_relpath),\n",
    "           'pivot_residues=%s' %(pivot_residues),\n",
    "           'chain=%s' %(chaintomove),\n",
    "           '-nstruct 5'\n",
    "          ]\n",
    "    \n",
    "    print 'Working on: %s %s' %(filenum, PDBID)\n",
    "    \n",
    "    outfile_path = os.path.join(predIDoutdir, 'rosetta.out')\n",
    "    rosetta_outfile = open(outfile_path, 'w')\n",
    "    print 'Running RosettaScript...'\n",
    "    rosetta_process = subprocess.Popen(arg, stdout=rosetta_outfile, cwd=predIDoutdir)\n",
    "    return_code = rosetta_process.wait()\n",
    "    print 'Task return code:', return_code, '\\n'\n",
    "    rosetta_outfile.close()    \n",
    "    \n",
    "    return filenum\n",
    "\n",
    "#Define paths\n",
    "outputdir = 'output/'\n",
    "\n",
    "#ACTION!!!\n",
    "chaintomove, inputdir = json_parser()\n",
    "#filenum = bash(chaintomove, inputdir, outputdir)\n",
    "print 'FINISHED!!!'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##RosettaOut_Parser.py\n",
    "This script parses DDG_Test.xml RosettaScript output and extracts individual and sum DDG energies into a bunch of nested dictionaries (which I understand is basically json format?).\n",
    "\n",
    "Goes through a filetree formatted like this:\n",
    "\n",
    "Working directory (specified by my_working_directory variable)<br>\n",
    "----PredictionID 1<br>\n",
    "--------rosetta.out<br>\n",
    "----PredictionID 2<br>\n",
    "--------rosetta.out<br>\n",
    "----PredictionID 3<br>\n",
    "--------rosetta.out<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Iterates through directories to open Rosetta.out and creates Nested Dictionaries:\n",
    "#>Prediction ID\n",
    "#>>Structure ID (nstruct 1-100)\n",
    "#>>>Structure Type (WT, Mut, DDG)\n",
    "#>>>>Score Type\n",
    "\n",
    "import os\n",
    "import linecache\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "#Parses rosetta.out and Runscript output and adds values to dicitonary fattydict\n",
    "#Counts unfinished jobs in dictionary unfinished\n",
    "def parse_rosetta_out(workingdir, verbose = True):\n",
    "    fattydict = {}\n",
    "    unfinished = {}\n",
    "\n",
    "    task_dirs = [os.path.join(workingdir, d) for d in os.listdir(workingdir) if os.path.isdir( os.path.join(workingdir, d) )]\n",
    "#    r = Reporter('parsing task directories', entries = 'directories')\n",
    "#    r.set_total_count( len(task_dirs) )\n",
    "\n",
    "#For each subdirectory (PredictionID) in the working directory (output/)...\n",
    "    for task_dir in task_dirs:\n",
    "        try:\n",
    "            i = long(os.path.basename(task_dir))\n",
    "        except ValueError:\n",
    "            print 'Directory %s is not a number' % i\n",
    "            continue\n",
    "        \n",
    "        fattydict[i] = {}\n",
    "        fattydict[i]['structIDs'] = {}\n",
    "        structID = 1\n",
    "        counter = 0\n",
    "        filename = os.path.join(task_dir, \"rosetta.out\")\n",
    "        \n",
    "        if not os.path.isfile( filename ):\n",
    "            print 'Missing output file:', filename\n",
    "            continue\n",
    "        \n",
    "        for line in enumerate(open(filename, 'r')):\n",
    "            #Finds first instance of 'fa_atr' and adds values to fattydict line-by-line until it reaches the dashed cutoff line\n",
    "            if line[1].find(\"fa_atr\") == 1:\n",
    "                if counter % 2 == 0:\n",
    "                    struct_type = 'WT'\n",
    "                else:\n",
    "                    struct_type = 'Mutant'\n",
    "\n",
    "                linecounter = 0\n",
    "                currentline = line[0] + 1\n",
    "                \n",
    "                if structID not in fattydict[i]['structIDs']:\n",
    "                    fattydict[i]['structIDs'][structID] = {}\n",
    "                fattydict[i]['structIDs'][structID][struct_type] = {}\n",
    "                \n",
    "                try:\n",
    "                    while linecache.getline(filename, currentline).strip() != '-----------------------------------------':\n",
    "                        scores = linecache.getline(filename, currentline)\n",
    "                        parsed_scores = scores.split()\n",
    "                        fattydict[i]['structIDs'][structID][struct_type][parsed_scores[0]] = parsed_scores[1]\n",
    "                        linecounter = linecounter + 1\n",
    "                        currentline = currentline + 1\n",
    "                    sumscore = linecache.getline(filename, line[0] + linecounter + 2)\n",
    "                    parsed_sumscore = sumscore.split()\n",
    "                    fattydict[i]['structIDs'][structID][struct_type][parsed_sumscore[0]] = parsed_sumscore[2]\n",
    "                    counter = counter + 1\n",
    "                except:\n",
    "                    print \"Oops, something went wrong here...\"\n",
    "\n",
    "            elif line[1].find(\"reported success in\") > 1:\n",
    "                timeline = line[1].split()\n",
    "                fattydict[i]['structIDs'][structID]['Runtime'] = timeline[5]\n",
    "                structID = structID +1\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        if verbose:\n",
    "            print str(i) + \": \" + str(structID - 1) + \" structures completed\"\n",
    "\n",
    "        #Keeps track of unfinished jobs (need to manually change how many files to expect)\n",
    "        if structID - 1 < 100:\n",
    "            unfinished[i] = structID - 1\n",
    "        else:\n",
    "            continue\n",
    "#        r.increment_report()\n",
    "#    r.done()\n",
    "\n",
    "\n",
    "#Parse output file for Max VMem usage (GB), start/end times, and return code\n",
    "        files = os.listdir(workingdir + str(i))\n",
    "        for doc in files:\n",
    "            if doc.startswith(\"SubmitRun_DDG_Zemu_General.py.o\"):\n",
    "                output = doc\n",
    "                print output\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        date_format_string = '%Y-%m-%d %H:%M:%S'\n",
    "        outputfile = workingdir + str(i) + \"/\" + output\n",
    "        for line in enumerate(open(outputfile, 'r')):\n",
    "            if line[1].startswith(\"Starting time:\"):\n",
    "                starting_time = line[1][15:].strip()\n",
    "                fattydict[i]['Starting time'] = datetime.datetime.strptime(starting_time, date_format_string)\n",
    "            elif line[1].startswith(\"Ending time:\"):\n",
    "                ending_time = line[1][13:].strip()\n",
    "                fattydict[i]['Ending time'] = datetime.datetime.strptime(ending_time, date_format_string)\n",
    "            elif line[1].startswith(\"Task return code:\"):\n",
    "                task_return_code = line[1].split()\n",
    "                fattydict[i]['Task return code'] = long(task_return_code[3])\n",
    "            elif line[1].startswith(\"Max virtual memory usage:\"):\n",
    "                mem_usage_parsed = line[1].split()\n",
    "                mem_usage = float(mem_usage_parsed[4][:-1])\n",
    "                size = mem_usage_parsed[4][-1:]\n",
    "                print size\n",
    "                if size == 'M':\n",
    "                    mem_usage = mem_usage / 1000\n",
    "                elif size == 'G':\n",
    "                    continue\n",
    "                else:\n",
    "                    print \"Memory usage not measured in MB or GB!\"\n",
    "                print mem_usage\n",
    "                fattydict[i]['Max virtual memory usage:'] = float(mem_usage)\n",
    "\n",
    "    return fattydict, unfinished\n",
    "\n",
    "def main():\n",
    "#    my_working_directory = str(os.getcwd() + '/')\n",
    "    my_working_directory = '/Users/jameslucas/Kortemme_Rotation/output/'\n",
    "    print my_working_directory\n",
    "    parsed_dict, unfinished_jobs = parse_rosetta_out(my_working_directory)\n",
    "\n",
    "    print parsed_dict\n",
    "    os.chdir(my_working_directory)\n",
    "\n",
    "#    open(\"DDG_Data.json\", \"w\").write(json.dumps(parsed_dict, sort_keys=True,separators=(',', ': ')))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##List_PDBs.py\n",
    "Does pretty much what it says, lists all of the .pdb files in subdirectories within a specificed directory (specified by workingdir variable). Probably should have used a set to make the list non-redundant, but I just took the output and sorted it in excel. :|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prints out all PDB files in subdirectories of workingdir\n",
    "#List_PDBs.py\n",
    "import os\n",
    "\n",
    "workingdir = '/netapp/home/james.lucas/160315-kyleb_james-backrub-rscript/data/'\n",
    "#workingdir = '/Users/jameslucas/Kortemme_Rotation/output/'\n",
    "for i in os.listdir(workingdir):\n",
    "    if os.path.isdir(workingdir+i):\n",
    "        for j in os.listdir(workingdir+i):\n",
    "            if j.endswith('.pdb'):\n",
    "                print j\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##PDB_REDO .pdb Scraper\n",
    "Made a non-redundant list of PDBs using List_PDBs.py and Excel (yay) to download from the PDB_REDO database. Takes each PDBID from the list and downloads the PDB_REDO version. Heads up, many of the PDB_REDO versions of the structures in the Zemu dataset do not exist. The script still reads the .csv and creates a blank .pdb file if the PDB_REDO version doesn't exist, so I did need to go back and toss out the empty .pdb files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Downloads all PDB_REDO's from a list\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "os.chdir('/Users/jameslucas/Kortemme_Rotation')\n",
    "cwd = os.getcwd()\n",
    "new_wd = cwd + '/PDB_REDO'\n",
    "\n",
    "try:\n",
    "    os.mkdir(new_wd)\n",
    "except:\n",
    "    print 'PDB_REDO already exists!'\n",
    "\n",
    "os.chdir(new_wd)\n",
    "\n",
    "df = pd.read_csv('/Users/jameslucas/Kortemme_Rotation/PDB_List.csv')\n",
    "\n",
    "for i,j in df.iterrows():\n",
    "    url = 'http://www.cmbi.ru.nl/pdb_redo/%s/%s/%s_final.pdb' %(j[0][1:-1],j[0],j[0])\n",
    "    r = requests.get(url)\n",
    "    print \"%s_%s\" % (j[0].upper(), j[1])\n",
    "    with open(\"%s_%s.pdb\" % (j[0].upper(), j[1]), \"wb\") as pdbfile:\n",
    "        pdbfile.write(r.content)\n",
    "\n",
    "print 'DONE!'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##JSON File Editor\n",
    "This scripts edits entries in a .json file to match the contents of a desired data directory. This was written to remove PredictionID directories from the Zemu dataset .json file with missing/nonexistent PDB_REDO structures. Takes the PDB name from a json (jsoninfo) and checks against directory containing PDB_REDO .pdb's (stuff). Continues over .json entry if PDB exists and deletes entry if PDB is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Removes unnecessary entries in json file based on existing PDB_REDO pdb's in a directory\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "#Initialize stuff\n",
    "stuff = '/Users/jameslucas/Kortemme_Rotation/PDB_REDO/'\n",
    "os.chdir(stuff)\n",
    "jsoninfo  = json.load(open(\"/Users/jameslucas/Kortemme_Rotation/data/blank_job_dict.json\"))\n",
    "del_list = []\n",
    "\n",
    "#Checks if PDB_REDO exists for each jsoninfo entry, adds unwanted entries to list\n",
    "for i in jsoninfo:\n",
    "    parsed = re.sub(\"/\", ' ', jsoninfo[i][\"input_file_list\"][0])\n",
    "    data, filenum, pdbfile = parsed.split()\n",
    "    if os.path.isfile(stuff+pdbfile):\n",
    "        continue \n",
    "    else:\n",
    "        del_list.append(i)\n",
    "\n",
    "#Sorts list for aesthetics\n",
    "del_list.sort()\n",
    "print del_list\n",
    "\n",
    "#Deletes unwanted entries from jsoninfo\n",
    "for item in del_list:\n",
    "    jsoninfo.pop(item)\n",
    "\n",
    "#Write updated jsoninfo to file\n",
    "open(\"blank_job_dict_updated.json\", \"w\").write(\n",
    "    json.dumps(jsoninfo, sort_keys=True,separators=(',', ': ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##PDB_REDO data setup\n",
    "For each entry in a .json file (jsoninfo), this scripts takes all of the information from the original Zemu dataset (source) sans .pdb and combines it with the correct PDB_REDO .pdb file from another directory (PDB_REDO) in a new destination folder (dest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Copies subdirectories into new data directory, ignores .pdb's\n",
    "#Copies and replaces REDO_PDB .pdb's into new data directory\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "jsoninfo = json.load(open(\"/netapp/home/james.lucas/PDB_REDO/blank_job_dict_updated.json\"))\n",
    "source = '/netapp/home/james.lucas/160322-james-backrub-rscript-full/data'\n",
    "dest = '/netapp/home/james.lucas/Zemu-PDB_REDO_Dataset/data'\n",
    "PDB_REDO = '/netapp/home/james.lucas/PDB_REDO/'\n",
    "for i in jsoninfo:\n",
    "    parsed = re.sub(\"/\", ' ', jsoninfo[i][\"input_file_list\"][0])\n",
    "    data, filenum, pdbfile = parsed.split()\n",
    "    #Copy Resfiles and stuff from source to PDB_REDO data directory\n",
    "    shutil.copytree(source + i, dest + i, ignore = shutil.ignore_patterns('*.pdb'))\n",
    "    #Copy PDB_REDO pdb's from PDB_REDO directory to new data directory\n",
    "    shutil.copy2(PDB_REDO + i + \"/\" + pdbfile, dest + i + \"/\" + pdbfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Resfile_Editor.py\n",
    "Literally iterates through all directories in /data and replaces 'NATRO' with 'NATAA' in mutation.resfile and dumps that into a new file mutation_repack.resfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Edits all resfiles to allow for repack only\n",
    "import os\n",
    "\n",
    "def resfile_editor(workingdir):\n",
    "    for i in os.listdir(workingdir):\n",
    "        if os.path.isdir(workingdir + i):\n",
    "            resfile = open(workingdir + i + '/mutations.resfile', 'r')\n",
    "            outfile = open(workingdir + i + '/mutations_repack.resfile', 'w')\n",
    "            for line in resfile:\n",
    "                if line.strip() == 'NATRO':\n",
    "                    outfile.write('NATAA\\n')\n",
    "                else:\n",
    "                    outfile.write(line)\n",
    "            outfile.close()\n",
    "            \n",
    "datapath = os.path.relpath('/Users/jameslucas/Kortemme_Rotation/data', os.getcwd())\n",
    "os.chdir(datapath)\n",
    "workingdir = os.getcwd() + '/'\n",
    "resfile_editor(workingdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Strip PDBs\n",
    "This script reads PDB IDs from a .csv (generated from List_PDBs.py, manually sorted for PDB_REDO only structures) and opens the original pdb file (pdb_original). It looks at the name to see which chains are needed and only writes those chains to a new PDB file (pdb_stripped)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def strip_pdbs(df):\n",
    "    for i in df.iterrows():\n",
    "        pdb_original = open(os.getcwd() + '/PDB_REDO/' + i[1]['PDB'].upper() + '_' + i[1]['Chains'] + '.pdb', 'r')\n",
    "        pdb_stripped = open(os.getcwd() + '/PDB_REDO_Stripped/' + i[1]['PDB'].upper() + '_' + i[1]['Chains'] + '.pdb', 'w')\n",
    "        for line in pdb_original:\n",
    "            parsed = line.split()\n",
    "            if parsed[0].strip() != 'ATOM':\n",
    "                pdb_stripped.write(line)\n",
    "            else:\n",
    "                print i[1]['Chains']\n",
    "                if parsed[4] in i[1]['Chains']:\n",
    "                    pdb_stripped.write(line)\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "df = pd.read_csv('PDB_List_REDO.csv')\n",
    "\n",
    "try:\n",
    "    os.mkdir('PDB_REDO_Stripped')\n",
    "except:\n",
    "    print 'PDB_REDO_Stripped already exists'\n",
    "\n",
    "strip_pdbs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Scratch Space\n",
    "An area to test bits of code and keep old cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "id = 99\n",
    "pwd = os.getcwd()\n",
    "print pwd\n",
    "\n",
    "new_id = '12345678' + str(id)\n",
    "new_id = new_id[-4:]\n",
    "print new_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = ['23A','34B','45C','56','67','78','89']\n",
    "\n",
    "targetlist = ''\n",
    "for i in target:\n",
    "    targetlist = targetlist + i + ','\n",
    "targetlist = targetlist[:-1]\n",
    "print targetlist\n",
    "\n",
    "import os\n",
    "print os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#qsub commands\n",
    "qsub\n",
    "qhold (job #)\n",
    "qrls (job #).(task #'s):1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Submit script!!!\n",
    "#OLD VERSION, NO LONGER IN USE\n",
    "\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from finalize_JL import find_neighbors\n",
    "from finalize_JL import read_mutations_resfile\n",
    "import subprocess\n",
    "\n",
    "#Parses dataset .json file and outputs chain to move and input PDB file directory\n",
    "def json_parser():\n",
    "    \n",
    "    asdf = open(\"/Users/jameslucas/Kortemme_Rotation/blank_job_dict_copy.json\")\n",
    "    jsonfile = json.load(asdf)\n",
    "    \n",
    "    key = sorted(jsonfile.keys())[0]\n",
    "    chaintomove = jsonfile[key][\"%%chainstomove%%\"]\n",
    "    directory = jsonfile[key]['input_file_list'][0]\n",
    "    \n",
    "    return chaintomove, directory\n",
    "\n",
    "#Finds neighbors within 8A and adds position and Chain information\n",
    "def neighbors_list(pdb_filepath, pdb_file):\n",
    "    neighbors = find_neighbors(\"/Users/jameslucas/Kortemme_Rotation/\"+pdb_filepath, \"/Users/jameslucas/Kortemme_Rotation/\" + pdb_file, 8)\n",
    "    pivotlist = ''\n",
    "    for i in neighbors:\n",
    "        string_parse = re.sub(\"[(),']\",'', str(i))\n",
    "        for s in string_parse.split():\n",
    "            if s.isdigit():\n",
    "                pivotlist = pivotlist + s\n",
    "            else:\n",
    "                pivotlist = pivotlist + s + ','\n",
    "                \n",
    "    pivotlist = pivotlist[:-1]\n",
    "    return pivotlist\n",
    "\n",
    "#Reads resfile and returns mutation position+chain and type\n",
    "def resfile_stuff(pdb_filepath):\n",
    "    resfile = read_mutations_resfile(\"/Users/jameslucas/Kortemme_Rotation/\"+pdb_filepath)\n",
    "    for i in resfile:\n",
    "        position = i[0]\n",
    "        chain = i[1]\n",
    "        mut_to = i[3]\n",
    "    return position, chain, mut_to\n",
    "    \n",
    "#Prints CMD input with PDBID, associated mutation, and pivot residues\n",
    "def bash(chaintomove, pdb_file):\n",
    "    #Working directory\n",
    "    workingdir = '/Users/jameslucas/Kortemme_Rotation/output/'\n",
    "    #Removes PDB file from path, saves in variable data_dir\n",
    "    pdb_file_parse = re.sub(\"/\",' ', str(pdb_file))\n",
    "    data, filenum, pdbtemp = pdb_file_parse.split()\n",
    "    data_dir = data + \"/\" + filenum\n",
    "    PDBID = pdbtemp[:-4]\n",
    "    \n",
    "    #Makes a folder for data dumping\n",
    "    os.mkdir(workingdir + filenum)\n",
    "    \n",
    "    #Dictionary: 1- to 3-letter code\n",
    "    res_dict = {\n",
    "        'A':'ALA',\n",
    "        'C':'CYS',\n",
    "        'D':'ASP',\n",
    "        'E':'GLU',\n",
    "        'F':'PHE',\n",
    "        'G':'GLY',\n",
    "        'H':'HIS',\n",
    "        'I':'ILE',\n",
    "        'K':'LYS',\n",
    "        'L':'LEU',\n",
    "        'M':'MET',\n",
    "        'N':'ASN',\n",
    "        'P':'PRO',\n",
    "        'Q':'GLN',\n",
    "        'R':'ARG',\n",
    "        'S':'SER',\n",
    "        'T':'THR',\n",
    "        'V':'VAL',\n",
    "        'W':'TRP',\n",
    "        'Y':'TYR'\n",
    "    }\n",
    "        \n",
    "    #Assigns function output to variables for bash input (pivot_residues, target, new_res)\n",
    "    target, chain, new_res_one = resfile_stuff(data_dir)\n",
    "    new_res_three = res_dict[new_res_one]\n",
    "    pivot_residues = neighbors_list(data_dir, pdb_file)\n",
    "    \n",
    "####All the variables and stuff for printing out the bash script\n",
    "    \n",
    "#Local Testing - All things\n",
    "\n",
    "    arg = ['/Users/jameslucas/rosetta_src_2016.02.58402_bundle/main/source/bin/rosetta_scripts.macosclangrelease',\n",
    "           '-s',\n",
    "           '/Users/jameslucas/Kortemme_Rotation/%s/%s.pdb' %(data_dir, PDBID),\n",
    "           '-parser:protocol',\n",
    "           '/Users/jameslucas/Kortemme_Rotation/DDG_Test.xml',\n",
    "           '-ignore_unrecognized_res',\n",
    "           '-out:path:pdb',\n",
    "           workingdir + filenum,\n",
    "           '-parser:script_vars',\n",
    "           'target=%s' %(target),\n",
    "           'new_res=%s' %(new_res_three),\n",
    "           'pivot_residues=%s' %(pivot_residues),\n",
    "           '-nstruct 5'\n",
    "          ]\n",
    "    \n",
    "    outfile_path = os.path.join(workingdir, 'rosetta.out')\n",
    "    rosetta_outfile = open(outfile_path, 'w')\n",
    "    rosetta_process = subprocess.Popen(arg, stdout=rosetta_outfile, cwd=workingdir)\n",
    "    rosetta_outfile.close()\n",
    "\n",
    "#    subprocess.call(arg)\n",
    "        \n",
    "chaintomove, pdb_file = json_parser()\n",
    "bash(chaintomove, pdb_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
